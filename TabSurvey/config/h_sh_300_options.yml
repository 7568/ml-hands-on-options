# General parameters
dataset: H_sh_300_options
model_name: XGBoost # LinearModel, KNN, SVM, DecisionTree, RandomForest, XGBoost, CatBoost, LightGBM, ModelTree
                # MLP, TabNet, VIME, TabTransformer, RLN, DNFNet, STG, NAM, DeepFM, SAINT
objective: binary # Don't change
# optimize_hyperparameters: True

use_gpu: False
# use one GPU parameters
gpu_index: 7
# use GPU data_parallel parameters
gpu_ids: [0, 1]
data_parallel: False



# Optuna parameters - https://optuna.org/
n_trials: 2
direction: maximize

# Cross validation parameters
num_splits: 5
shuffle: True
seed: 221 # Don't change

# Preprocessing parameters
scale: False
target_encode: False
one_hot_encode: False

# Training parameters
batch_size: 128
val_batch_size: 256
early_stopping_rounds: 20
epochs: 1000
logging_period: 100

# About the data
num_classes: 1  # for classification
num_features: 185
